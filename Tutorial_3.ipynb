{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyasha-n-m/OT-tutorials/blob/main/Tutorial_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBr5AhjDVbhw"
      },
      "source": [
        "## Hands-on Tutorials on Computational Optimal Transport\n",
        "### Instructor: Nataliia Monina\n",
        "-----\n",
        "\n",
        "### 3. ML for OT\n",
        "In this notebook, we'll consider an example of a use of ML in OT (parametrization of dual potentials)\n",
        "\n",
        "\n",
        "1. Consider an implementation of a particular algorithm proposed by Meta (MetaOT)\n",
        "2. Implement other algorithms that will use a Deep Neural Network to estimate OT for entropic and for other convex regularization. See how they perform (do some ideas even work?)\n",
        "3. Special exercise (More work -- optional): We may also consider a way to involve generating a lot of training data in a fast way (both marginals and potentials) and create a supervised learning model!\n",
        "\n",
        "\n",
        "**Important comment:** In parts 1-2 of this tutorial, we will only train the network on a **single** sample. The general idea is to use the NN as a way to parametrize OT potential. We can simply view it as another way of optimization, although it may be like an \"overshooting\".\n",
        "\n",
        "*I apologize in advance for possible noodles-code :)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pfY8SiI22Ev"
      },
      "outputs": [],
      "source": [
        "!pip install POT --quiet\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ot\n",
        "import ot.plot\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBQGO2wj4-XG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda, Flatten, Conv2D, Concatenate\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXRS2I2Iql5P"
      },
      "outputs": [],
      "source": [
        "# helpful function to plot and analyze our solutions\n",
        "def analyze_solutions(mu, nu, P_computed, P_for_compare, epsilon, C):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu: np.ndarray\n",
        "        Actual measure mu\n",
        "    nu: np.ndarray\n",
        "        Actual measure nu\n",
        "    P_computed: np.ndarray\n",
        "        Computed plan P with the chosen method\n",
        "    P_for_compare: np.ndarray\n",
        "        Reference plan P to compare with (e.g., output of ot.emd)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    plt.figure\n",
        "        Plot of comparisons for our computed solution\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # error of marginals\n",
        "    # mu vs. output\n",
        "    ax[0].plot(mu)\n",
        "    ax[0].plot(np.sum(P_computed, axis=1))\n",
        "    ax[0].set_title('mu vs P_computed_mu')\n",
        "    # nu vs. output\n",
        "    ax[1].plot(nu)\n",
        "    ax[1].plot(np.sum(P_computed, axis=0))\n",
        "    ax[1].set_title('nu vs P_computed_nu')\n",
        "\n",
        "    # comparison of plans\n",
        "    ax[2].imshow(P_computed)\n",
        "    ax[2].set_title(f\"P_computed (eps={epsilon})\")\n",
        "    ax[3].imshow(P_for_compare)\n",
        "    ax[3].set_title(f\"P_for_compare\")\n",
        "\n",
        "    print(\"Cost of P_computed:\", np.einsum('ij,ij', C, P_computed))\n",
        "    print(\"Cost of P_for_compare:\", np.einsum('ij,ij', C, P_for_compare))\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grGz76M_6zUI"
      },
      "outputs": [],
      "source": [
        "X_1 = np.linspace(0,4, 51)\n",
        "Y_1 = np.linspace(0,4, 61)\n",
        "\n",
        "def discrete_gaussian(mean, variance, interval):\n",
        "    f = np.exp(-(interval-mean)**2/(2*variance)) / np.sqrt(2*np.pi*variance)\n",
        "    return f/f.sum()\n",
        "\n",
        "mu_1 = discrete_gaussian(1, 0.1, X_1)\n",
        "nu_1 = discrete_gaussian(2.8, 0.06, Y_1)\n",
        "\n",
        "'''Test measures'''\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(X_1, mu_1, 'o', label=\"mu_1\")\n",
        "plt.plot(Y_1, nu_1, '+', label=\"nu_1\")\n",
        "plt.legend(loc=0)\n",
        "\n",
        "'''Cost: Distance squared '''\n",
        "\n",
        "C_1 = (X_1[:, None] - Y_1[None, :]) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_bPtMSH4SdB"
      },
      "source": [
        "## Machine Learning approach for computing OT:\n",
        "\n",
        "The goal is to train a neural network that takes as input a pair of marginal distributions $\\mu$ and $\\nu$, and outputs the dual potentials $U$ and $V$ (discretized) that solve the entropically regularized dual OT problem.\n",
        "\n",
        "### Problem Setup\n",
        "We consider the regularized OT problem with $\\varepsilon > 0$ and convex functions $\\psi$.\n",
        "\n",
        "$$\\max \\{ D^{\\varepsilon}_C(u,v) = \\sum_{i=0}^{n-1} u_i \\mu_i + \\sum_{j=n}^{m-1} v_j \\nu_j - \\varepsilon \\sum_{i=0}^{n-1}\\sum_{j=0}^{m-1} \\psi({\\frac{u_i+v_j - C_{ij}}{\\varepsilon}}) \\,:\\, u\\in \\mathbb{R}^n,~ v\\in \\mathbb{R}^m\\}$$\n",
        "\n",
        "Instead of solving it via classical optimization, we learn an approximation by computing it with a DNN:\n",
        "\n",
        "### Loss Function\n",
        "The loss function can be chosen in several ways.\n",
        "- `supervised` learning: pretrain some data with true solutions, and then use standard loss like MSE (we will not consider it today).\n",
        "- `unsupervised` learning: choose as loss, for example, the (negative) dual functional -- since we want to maximize it anyway!\n",
        "\n",
        "$$ L = -\\frac{1}{\\sharp samples} \\sum_{k=1}^{\\sharp samples} D(U^k, V^k)$$\n",
        "\n",
        "\n",
        "\n",
        "This ML-based approach is is promised to be powerful when we need to solve OT for many different pairs of marginals quickly â€” for instance, in generative modeling, meta-learning, or amortized inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWZPqycZ5OS-"
      },
      "outputs": [],
      "source": [
        "# useful re-declaratons of some of functions in language of tensorflow\n",
        "\n",
        "\n",
        "def tf_psi_0(x):\n",
        "    return tf.exp(x)\n",
        "\n",
        "def tf_psi_1(x):\n",
        "    return tf.square(tf.math.maximum(x, tf.constant([0.])))\n",
        "\n",
        "def tf_psi_2(x):\n",
        "    return tf.math.log(tf.exp(x) + 1)\n",
        "\n",
        "def tf_psi_0_prime(x):\n",
        "    return tf.exp(x)\n",
        "\n",
        "def tf_psi_1_prime(x):\n",
        "    return tf.math.maximum(x, tf.constant([0.]))\n",
        "\n",
        "def tf_psi_2_prime(x):\n",
        "    return 1 /(tf.exp(-x) + 1)\n",
        "\n",
        "def compute_U_V_C(U, V, C):\n",
        "    \"\"\"Calculates in appropriate dimensions along the batch:\n",
        "    .. math::\n",
        "        U_i \\oplus V_j - C_ij\n",
        "\n",
        "    \"\"\"\n",
        "    if len(U.shape) == 1:\n",
        "        return U[:, None] + V[None, :] - C\n",
        "    U_ = U[:, :, tf.newaxis]                # shape (n_samples, len(mu), 1)\n",
        "    V_ = V[:, tf.newaxis, :]                # shape (n_samples, 1, len(nu))\n",
        "    C_ = C[tf.newaxis, :, :]                # shape (1, len(mu), len(nu))\n",
        "    U_V_C = U_ + V_ - C_\n",
        "    return U_V_C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CxmTto800E1"
      },
      "source": [
        "### Meta OT\n",
        "\n",
        "In this [paper](https://arxiv.org/abs/2206.05262), the group of researchers in Meta proposed the following idea of using DNN for solving OT.\n",
        "\n",
        "Particular choice of regularization:\n",
        "- choose $\\psi=exp$, and thus we know there are easy conditions for checking marginals and potentials explicitly\n",
        "- the network needs only to predict one of potentials (e.g., `U`), and then compute the other one by the half-step of Sinkhorn:\n",
        "\n",
        "      V = eps * log(nu) - eps * log(sum(exp((U[:, None] - C) / eps), axis=0))\n",
        "\n",
        "(This can be simply extended to a multi-marginal setting as well: predict N-1 potentials, and compute the missing one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9TtcpLiwlLF"
      },
      "outputs": [],
      "source": [
        "\"\"\"Meta OT Network: possible implementation\"\"\"\n",
        "\n",
        "\n",
        "class OT_Sinkhorn_network:\n",
        "    \"\"\"\n",
        "    Builds a MetaOT model that maps (mu, nu) -> (U, V(U))\n",
        "    Each of mu and nu has length `mu_len` and `nu_len` (e.g., (n,m))\n",
        "    This network essentially predicts a good U, and then computes V = V(U)\n",
        "    V(U) is computed via a half-Sinkhorn step\n",
        "\n",
        "    loss:  ses loss function = -D(U, V(U))\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, mu_len, nu_len, C, eps):\n",
        "        self.mu_len = mu_len\n",
        "        self.nu_len = nu_len\n",
        "        self.C = C\n",
        "        self.eps = eps\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "        self.history = []\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        # Inputs: two marginals\n",
        "        mu_input = Input(shape=(self.mu_len, ), name=\"mu\")\n",
        "        nu_input = Input(shape=(self.nu_len, ), name=\"nu\")\n",
        "\n",
        "        # Concatenate inputs\n",
        "        x = Concatenate(name=\"concat\")([mu_input, nu_input])\n",
        "\n",
        "        hidden1 = Dense(64, activation='relu', kernel_initializer='zeros')(x)\n",
        "        drop1 = Dropout(0.2)(hidden1)\n",
        "        hidden2 = Dense(64, activation='relu', kernel_initializer='zeros')(drop1)\n",
        "        hidden3 = Dense(64, activation='relu', kernel_initializer='zeros')(hidden2)\n",
        "        drop2 = Dropout(0.1)(hidden3)\n",
        "\n",
        "        # Two outputs: U and V (same shape as inputs)\n",
        "        U_output = Dense(self.mu_len, name=\"U\", kernel_initializer='zeros',\n",
        "                         bias_initializer='zeros')(drop2)\n",
        "\n",
        "        V_output = Lambda(lambda x: self.compute_V(x[0], x[1]), name=\"V\")([nu_input, U_output])\n",
        "\n",
        "        model = Model(inputs=[mu_input, nu_input], outputs=[U_output, V_output])\n",
        "        return model\n",
        "\n",
        "    def compute_V(self, nu, U):\n",
        "        U_ = U[:, :, tf.newaxis]\n",
        "        C_ = self.C[tf.newaxis, :, :]\n",
        "        nu_float = tf.cast(nu, tf.float32)\n",
        "\n",
        "        V = tf.math.log(nu_float) - tf.math.log(tf.reduce_sum(tf.exp((U_ - C_)/self.eps), axis=1))\n",
        "        return self.eps * V\n",
        "\n",
        "    def dual(self, mu, nu): # quadratic_dual value on the batch\n",
        "        U, V = self.model.call({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)})\n",
        "        U_V_C = compute_U_V_C(U, V, self.C)\n",
        "        P = tf.exp(U_V_C/ self.eps)\n",
        "        D = - self.eps*tf.reduce_sum(P, axis=range(1,3))\n",
        "        D += tf.reduce_sum(U*mu, axis=1)\n",
        "        D += tf.reduce_sum(V*nu, axis=1)\n",
        "        return D\n",
        "\n",
        "    def loss(self, mu, nu):\n",
        "        return  -tf.reduce_sum(self.dual(mu, nu))\n",
        "\n",
        "    def train_step(self, mu, nu): #optimization step\n",
        "        with tf.GradientTape() as tape: #assign loss function\n",
        "            loss = self.loss(mu, nu)\n",
        "        self.history.append(loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, mu, nu, epochs=1000):\n",
        "        for i in range(epochs):\n",
        "            loss = self.train_step(mu, nu)\n",
        "\n",
        "    def call(self, mu, nu):\n",
        "        return self.model.call(({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)}))\n",
        "\n",
        "    def predict_P(self, mu, nu):\n",
        "        U, V = self.call(mu, nu)\n",
        "        if len(U.shape) == 1:\n",
        "            return np.exp((U[:, None] + V[None, :] - self.C) / self.eps)\n",
        "\n",
        "        P = tf.exp(compute_U_V_C(U,V, self.C) / self.eps)\n",
        "        return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1Taod9kwlWb"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "sink_model = OT_Sinkhorn_network(len(mu_1), len(nu_1), C_1, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAP8ks_9zigA"
      },
      "outputs": [],
      "source": [
        "sink_model.fit(mu_1[None, :], nu_1[None, :], epochs=300)\n",
        "\n",
        "P = sink_model.predict_P(mu_1[None, :], nu_1[None, :])[0]\n",
        "plt.plot(sink_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6oOw5ZXb1mY"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P, P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "- Try to understand the general approach, maybe do by-hand computations if needed\n",
        "- Try to generate some more sample measures (you can see an example of generating a batch at ths bottom of tutorial) and try to train the network on several train measures. Do you think one needs to adapt and change the network architechture or maybe something with a loss functions?\n",
        "- Play around with different pairs of measuses, values of `epsilon`, number of layers, different activation functions and so on -- you have a lot of freedom in this"
      ],
      "metadata": {
        "id": "VCKjvEp4m6rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Free space for experiments\"\"\""
      ],
      "metadata": {
        "id": "d6hJDc_on7sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN with general regularization\n",
        "\n",
        "Let's now try to implement a network as described in the idea in the beginning of the tutorial"
      ],
      "metadata": {
        "id": "VD_IYWTbn_v1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFPKSzfj4RMA"
      },
      "outputs": [],
      "source": [
        "class OT_psi_network:\n",
        "    \"\"\"\n",
        "    Builds a model that maps (mu, nu) -> (U, V)\n",
        "    Each of mu and nu has length `mu_len` and `nu_len` (e.g., (n,m))\n",
        "    \"\"\"\n",
        "    def __init__(self, mu_len, nu_len, C, eps, psi):\n",
        "        self.mu_len = mu_len\n",
        "        self.nu_len = nu_len\n",
        "        self.C = C\n",
        "        self.eps = eps\n",
        "        self.psi = psi\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "        self.history = []\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        # Inputs: two marginals\n",
        "        mu_input = Input(shape=(self.mu_len, ), name=\"mu\")\n",
        "        nu_input = Input(shape=(self.nu_len, ), name=\"nu\")\n",
        "\n",
        "        # Concatenate inputs\n",
        "        x = Concatenate(name=\"concat\")([mu_input, nu_input])\n",
        "\n",
        "        # Internal hidden layers\n",
        "        hidden1 = Dense(64, activation='sigmoid', kernel_initializer='zeros')(x)\n",
        "        drop1 = Dropout(0.2)(hidden1)\n",
        "        hidden2 = Dense(64, activation='relu', kernel_initializer='zeros')(drop1)\n",
        "        hidden3 = Dense(64, activation='relu', kernel_initializer='zeros')(hidden2)\n",
        "        drop2 = Dropout(0.1)(hidden3)\n",
        "\n",
        "        # Two outputs: U and V (same shape as inputs)\n",
        "        U_output = Dense(self.mu_len, name=\"U\", kernel_initializer='zeros',\n",
        "                         bias_initializer='zeros')(drop2)\n",
        "        V_output = Dense(self.nu_len, name=\"V\", kernel_initializer='zeros',\n",
        "                         bias_initializer='zeros')(drop2)\n",
        "\n",
        "        model = Model(inputs=[mu_input, nu_input], outputs=[U_output, V_output])\n",
        "        return model\n",
        "\n",
        "    def psi_dual(self, mu, nu): # quadratic_dual value on the batch\n",
        "        U, V = self.model.call({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)})\n",
        "        U_V_C = compute_U_V_C(U, V, self.C)\n",
        "        psi_U_V_C = self.psi(U_V_C/ self.eps)\n",
        "        D = - self.eps*tf.reduce_sum(psi_U_V_C, axis=range(1,3))\n",
        "        D += tf.reduce_sum(U*mu, axis=1)\n",
        "        D += tf.reduce_sum(V*nu, axis=1)\n",
        "        return D\n",
        "\n",
        "    def loss(self, mu, nu):\n",
        "        return  -tf.reduce_sum(self.psi_dual(mu, nu))\n",
        "\n",
        "    def train_step(self, mu, nu, optimizer): #optimization step\n",
        "        with tf.GradientTape() as tape: #assign loss function\n",
        "            loss = self.loss(mu, nu)\n",
        "        self.history.append(loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, mu, nu, epochs=1000, lr=0.05):\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        for i in range(epochs):\n",
        "            loss = self.train_step(mu, nu, optimizer)\n",
        "\n",
        "    def call(self, mu, nu):\n",
        "        return self.model.call(({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)}))\n",
        "\n",
        "    def predict_P(self, mu, nu, psi_prime):\n",
        "        U, V = self.model.call({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)})\n",
        "        P = psi_prime(compute_U_V_C(U, V, self.C) / self.eps)\n",
        "        return P\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "psi_0"
      ],
      "metadata": {
        "id": "IqerhCiMvo3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RCGuogPMJz2"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "psi_model = OT_psi_network(len(mu_1), len(nu_1), C_1, epsilon, psi=tf_psi_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzpC6NHo84SF"
      },
      "outputs": [],
      "source": [
        "psi_model.fit(mu_1[None, :],nu_1[None, :], epochs=800)\n",
        "P = psi_model.predict_P(mu_1[None, :], nu_1[None, :], tf_psi_0_prime)\n",
        "\n",
        "plt.plot(psi_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY5tOUohNdOq"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P[0], P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCs1Xr8m9bjL"
      },
      "source": [
        "psi_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4PoxBJ59cwh"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "psi_model = OT_psi_network(len(mu_1), len(nu_1), C_1, epsilon, psi=tf_psi_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3RdYXTN9cwj"
      },
      "outputs": [],
      "source": [
        "psi_model.fit(mu_1[None, :], nu_1[None, :], epochs=3000, lr=0.03)\n",
        "P = psi_model.predict_P(mu_1[None, :], nu_1[None, :], tf_psi_1_prime)[0]\n",
        "\n",
        "plt.plot(psi_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THGGz69o9cwj"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P, P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "psi_2"
      ],
      "metadata": {
        "id": "7XTdqRa7wi_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgacM70mwos7"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.01\n",
        "psi_model = OT_psi_network(len(mu_1), len(nu_1), C_1, epsilon, psi=tf_psi_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4gS6ed6wos8"
      },
      "outputs": [],
      "source": [
        "psi_model.fit(mu_1[None, :], nu_1[None, :], epochs=900, lr=0.05)\n",
        "P = psi_model.predict_P(mu_1[None, :], nu_1[None, :], tf_psi_2_prime)[0]\n",
        "\n",
        "plt.plot(psi_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdv1Qhufwos9"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P, P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "- (As you may guess) Try to experiment with a different number of epochs, learning rates, etc\n",
        "- (Advanced) try to implement some sort of a simulation of a half-sinkhorn to imitate the idea of MetaOT, e.g. estimate only `U` and then compute `V` as some approximation of $argmax \\{D(U, \\cdot)\\}$ by a lambda layer -- you can even write a separate NN to estimate best V from (mu, nu, U), or directly with some nested gradient method. (Talk to me if interested)"
      ],
      "metadata": {
        "id": "U3B_5XS1xa7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csJkHPrXpYBl"
      },
      "outputs": [],
      "source": [
        "\"\"\"Free space for experiments\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens if we change a loss function and concentrate more on finding the \"satisfying marginal constraints\"\n",
        "\n",
        "Let's not try to write a network that will predict (`U`, `V`) with the condition that the marginals must be satisfied.\n",
        "\n",
        "We know that the optimal plan `P` must be of a form\n",
        "$$P_{ij} = \\psi'({\\frac{u_i+v_j - C_{ij}}{\\varepsilon}})$$\n",
        "\n",
        "Then we can then choose a loss of a form\n",
        "$$L = \\sum_i||\\mu_i-\\sum_j P_{ij}||^2 + \\sum_j||\\nu_j-\\sum_i P_{ij}||^2$$\n",
        "\n",
        "Question: How well does it actually perform?"
      ],
      "metadata": {
        "id": "H8SGI9bUzTCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbgzQ2hHpYLc"
      },
      "outputs": [],
      "source": [
        "class OT_psi_marginal_network:\n",
        "    \"\"\"\n",
        "    Builds a model that maps (mu, nu) -> (U, V)\n",
        "    Each of mu and nu has length `mu_len` and `nu_len` (e.g., (n,m))\n",
        "    Uses loss - error of marginals\n",
        "    \"\"\"\n",
        "    def __init__(self, mu_len, nu_len, C, eps, psi_prime):\n",
        "        self.mu_len = mu_len\n",
        "        self.nu_len = nu_len\n",
        "        self.C = C\n",
        "        self.eps = eps\n",
        "        self.psi_prime = psi_prime\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "        self.history = []\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        # Inputs: two marginals\n",
        "        mu_input = Input(shape=(self.mu_len, ), name=\"mu\")\n",
        "        nu_input = Input(shape=(self.nu_len, ), name=\"nu\")\n",
        "\n",
        "        # Concatenate inputs\n",
        "        x = Concatenate(name=\"concat\")([mu_input, nu_input])\n",
        "\n",
        "        hidden1 = Dense(64, activation='relu', kernel_initializer='zeros')(x)\n",
        "        hidden2 = Dense(64, activation='relu', kernel_initializer='zeros')(hidden1)\n",
        "        hidden3 = Dense(64, activation='relu', kernel_initializer='zeros')(hidden2)\n",
        "        hidden4 = Dense(64, activation='relu', kernel_initializer='zeros')(hidden3)\n",
        "\n",
        "\n",
        "        # Two outputs: U and V (same shape as inputs)\n",
        "        U_output = Dense(self.mu_len, name=\"U\", kernel_initializer='zeros',\n",
        "                         bias_initializer='zeros')(hidden4)\n",
        "        V_output = Dense(self.nu_len, name=\"V\", kernel_initializer='zeros',\n",
        "                         bias_initializer='zeros')(hidden4)\n",
        "\n",
        "        model = Model(inputs=[mu_input, nu_input], outputs=[U_output, V_output])\n",
        "        return model\n",
        "\n",
        "    def marginal_loss(self, mu, nu):\n",
        "        P = self.predict_P(mu, nu)\n",
        "        P_x = tf.reduce_sum(P, axis=2)\n",
        "        P_y = tf.reduce_sum(P, axis=1)\n",
        "\n",
        "        l = tf.reduce_sum(tf.square(mu - P_x))\n",
        "        l += tf.reduce_sum(tf.square(nu - P_y))\n",
        "        return l\n",
        "\n",
        "    def train_step(self, mu, nu): #optimization step\n",
        "        with tf.GradientTape() as tape: #assign loss function\n",
        "            loss = self.marginal_loss(mu, nu)\n",
        "        self.history.append(loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, mu, nu, epochs=1000):\n",
        "        for i in range(epochs):\n",
        "            loss = self.train_step(mu, nu)\n",
        "\n",
        "    def call(self, mu, nu):\n",
        "        return self.model.call(({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)}))\n",
        "\n",
        "    def predict_P(self, mu, nu):\n",
        "        U, V = self.model.call({\"mu\": tf.Variable(mu), \"nu\": tf.Variable(nu)})\n",
        "        P = self.psi_prime(compute_U_V_C(U,V, self.C) / self.eps)\n",
        "        return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_x8HDuss7Mg"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "test_model = OT_psi_marginal_network(len(mu_1), len(nu_1), C_1, epsilon, tf_psi_0_prime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX3se0yFcsAt"
      },
      "outputs": [],
      "source": [
        "test_model.fit(mu_1[None, :],nu_1[None, :],epochs=100)\n",
        "\n",
        "P = test_model.predict_P(mu_1[None, :], nu_1[None, :])\n",
        "plt.figure()\n",
        "plt.plot(test_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.history[-5:]"
      ],
      "metadata": {
        "id": "_CqHDzXd3k4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BSepzrqs_XN"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P[0], P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xik6xDrswlHT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "psi_1 maybe?"
      ],
      "metadata": {
        "id": "iL4ul8_d7Hfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AteqgF457GUu"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "test_model = OT_psi_marginal_network(len(mu_1), len(nu_1), C_1, epsilon, tf_psi_1_prime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXQyMATK7GUx"
      },
      "outputs": [],
      "source": [
        "test_model.fit(mu_1[None, :],nu_1[None, :],epochs=100)\n",
        "\n",
        "P = test_model.predict_P(mu_1[None, :], nu_1[None, :])\n",
        "plt.figure()\n",
        "plt.plot(test_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.history[-5:]"
      ],
      "metadata": {
        "id": "m9ic2MUY7GU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix8D6LC67GU0"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P[0], P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "psi_2 last chance?"
      ],
      "metadata": {
        "id": "EoN5_pES7Y-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opOKRKIK7enr"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "test_model = OT_psi_marginal_network(len(mu_1), len(nu_1), C_1, epsilon, tf_psi_2_prime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smfqJd9b7ent"
      },
      "outputs": [],
      "source": [
        "test_model.fit(mu_1[None, :],nu_1[None, :],epochs=100)\n",
        "\n",
        "P = test_model.predict_P(mu_1[None, :], nu_1[None, :])\n",
        "plt.figure()\n",
        "plt.plot(test_model.history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.history[-5:]"
      ],
      "metadata": {
        "id": "mi0S852Z7enu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kEl0ji47enu"
      },
      "outputs": [],
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P[0], P_pot_sink, epsilon, C_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may have observed, the performance of this loss function is not so good, why do you think so? (Talk to me)"
      ],
      "metadata": {
        "id": "stcfX_7w8V6X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnQkdBtxQgii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQol7WDMQgyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5zB8e2A1Xyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "# Time-consuming (optional) exercise\n",
        "\n",
        "Motivation: For entropy regularized OT, we know that for $(U,V)$ to be the maximizers of the dual problem, the following conditions must be satisfied:\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\mu_i = \\sum_{j}\\exp(\\frac{U_i + V_j - C_{ij}}{\\varepsilon}) & \\forall i\\\\\n",
        "\\nu_j = \\sum_{i}\\exp(\\frac{U_i + V_j - C_{ij}}{\\varepsilon}) & \\forall j\n",
        "\\end{cases}\n",
        "$$\n",
        "or, equivalently\n",
        "$$\n",
        "\\begin{cases}\n",
        "U_i = \\varepsilon(\\log \\mu_i - \\log \\sum_{j} \\exp(\\frac{V_j - C_{ij}}{\\varepsilon})) & \\forall i\\\\\n",
        "V_j = \\varepsilon(\\log \\nu_j - \\log \\sum_{i} \\exp(\\frac{U_i - C_{ij}}{\\varepsilon})) & \\forall j\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Then try to implement the following idea:\n",
        "Fix some domains $X = \\{x_1,...,x_N\\}$ and $Y = \\{y_1, ..., y_M\\}$ and compute a cost $C_{ij} = c(x_i, y_j)$ for some chosen cost function $c$, and choose $\\varepsilon >0$. Now:\n",
        "\n",
        "1. Generate $K$ samples of measures $\\mu^1, ..., \\mu^K$, e.g., as some gaussians or mixed gaussians (example of code below)\n",
        "2. Generate also $K$ potentials $V^1, ..., V^K$, e.g., some \"gaussians\", \"radial decays\" and so on\n",
        "3. For each $k=1,...,K$ compute\n",
        "$$\n",
        "U^k_i = \\varepsilon(\\log \\mu^k_i - \\log \\sum_{j} \\exp(\\frac{V^k_j - C_{ij}}{\\varepsilon}))\n",
        "$$\n",
        "4. For each $k=1,...,K$ compute\n",
        "$$\\nu^k_j = \\sum_{i}\\exp(\\frac{U^k_i + V^k_j - C_{ij}}{\\varepsilon})\n",
        "$$\n",
        "\n",
        "This way, we have generated a dataset of pairs of measures $(\\mu^k, \\nu^k)$ and corresponding $(U^k, V^k)$ which are in fact the solutions of the OT problems.\n",
        "\n",
        "Now, use this dataset for training a `supervised` Neural Network that takes as input $(\\mu, \\nu)$ and outputs $(U,V)$ which are the estimates for solutions of $OT_\\varepsilon(\\mu,\\nu)$. In this case, one can also use as loss MSE, or a mix of MSE and Dual formula loss (e.g., evaluation of some $-D_\\varepsilon(U_{sample}, V_{sample}$)).\n",
        "\n",
        "*(In the code snippets below, you are free to modify all the definitions and models of your `mu`'s and `V`'s, I am just adding a possible idea. I generally suggest that your potential $V$ should be decaying fast outside of some small regions (Ask me why if needed). You can plot your obtained missing `U`'s and `nu`'s and also `P`'s to see if your test samples are actually meaningful.)*\n",
        "\n",
        "------\n",
        "\n",
        "Have fun!\n"
      ],
      "metadata": {
        "id": "YpqxZd8c9Anz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example of a way to generate some test measures\"\"\"\n",
        "\n",
        "# generating some samples of measures (we will generate gaussian mixtures)\n",
        "# we will consider them all on the same domain\n",
        "low = -5\n",
        "high = 5\n",
        "n_discretization = 51\n",
        "interval = np.linspace(low, high, n_discretization)\n",
        "C1 = (interval[:, None] - interval[None, :])**2\n",
        "C1 /= np.sum(C1) # to control the values\n",
        "border_percent = 0.1 # make sure that mean of gaussians are a bit away from corners of the interval\n",
        "border_value = border_percent * (high - low)\n",
        "n_samples_per_mix = 20 # number of elements in gaussian mixture\n",
        "\n",
        "\n",
        "def gaussian_mixture(means, variances, interval):\n",
        "    mix_many = np.exp(-(interval[None, None, :]-means[:,:,None])**2/(2*variances[:,:,None])) / np.sqrt(2*np.pi*variances[:,:,None])\n",
        "    mix = np.sum(mix_many, axis=1)\n",
        "    return mix / (np.sum(mix, axis=1)[:, None])\n",
        "\n",
        "def radial_mixture(centers, rates, interval):\n",
        "    mix_many = np.exp(-rates[:, :, None] * np.abs(interval[None, None, :]-centers[:,:,None]))\n",
        "    mix = np.sum(mix_many, axis=1)\n",
        "    return mix / (np.sum(mix, axis=1)[:, None])\n",
        "\n",
        "\n",
        "mu_samples = []\n",
        "for i in range(1,6):\n",
        "    np.random.seed(i)\n",
        "    means = np.random.uniform(low + border_value, high - border_value, (n_samples_per_mix, i))\n",
        "    variances = np.random.uniform(0.05, 0.5, (n_samples_per_mix, i))\n",
        "    mu_samples.append(gaussian_mixture(means, variances, interval))\n",
        "\n",
        "mu_samples = np.vstack(mu_samples)\n",
        "\n",
        "np.random.seed(1)\n",
        "np.random.shuffle(mu_samples)\n",
        "\n",
        "V_samples = []\n",
        "for i in range(1,2): # i.e. gaussian-looking\n",
        "    np.random.seed(2*i)\n",
        "    means = np.random.uniform(low + border_value, high - border_value, (n_samples_per_mix, i))\n",
        "    variances = np.random.uniform(0.01, 0.5, (n_samples_per_mix, i))\n",
        "    V_samples.append(gaussian_mixture(means, variances, interval))\n",
        "\n",
        "for i in range(1,5): # i.e. radial decay\n",
        "    np.random.seed(3*i)\n",
        "    centers = np.random.uniform(low + border_value, high - border_value, (n_samples_per_mix, i))\n",
        "    rates = np.random.uniform(0.1, 1, (n_samples_per_mix, i))\n",
        "    V_samples.append(radial_mixture(centers, rates, interval))\n",
        "\n",
        "V_samples = np.vstack(V_samples)"
      ],
      "metadata": {
        "id": "8aeXrQ2PCBik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.001\n",
        "\n",
        "K1 = np.exp(- C1 / (eps * np.max(C1)))\n",
        "P_u_j = np.exp((V_samples[:, None, :])/ eps) * K1[None, :, :]\n",
        "\n",
        "U_samples = eps * np.log(mu_samples) - eps * np.log(np.sum(P_u_j, axis=2))\n",
        "\n",
        "P_samples = np.exp((U_samples[:, :, None] + V_samples[:, None, :])/ eps) * K1[None, :, :]\n",
        "nu_samples = np.sum(P_samples, axis=1)\n"
      ],
      "metadata": {
        "id": "Cpeoqj0eoZrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(interval, V_samples[-20:].T)"
      ],
      "metadata": {
        "id": "2dFxj_PTqJMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a look at one of the sample problems and solutions\n",
        "idx = 95\n",
        "plt.figure()\n",
        "# OT-eps plan\n",
        "ot.plot.plot1D_mat(mu_samples[idx], nu_samples[idx], P_samples[idx])\n",
        "plt.figure()\n",
        "# compared to product measure\n",
        "ot.plot.plot1D_mat(mu_samples[idx], nu_samples[idx], mu_samples[idx, :, None]*nu_samples[idx, None, :])"
      ],
      "metadata": {
        "id": "dYoFy1Rgxy9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(V_samples[idx])"
      ],
      "metadata": {
        "id": "JFaDbZ6YyCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Work on your Neural Network here\"\"\""
      ],
      "metadata": {
        "id": "iMP0Udbjzwsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjyI9VEPz5tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YiD-yQMhz5vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzXGMJgWz5yA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxWKOWhI2ow7IXxJ61IcGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}