{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp7A9991gISbEgPsDe1VbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manyasha-n-m/OT-tutorials/blob/main/Tutorial_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on Tutorials on Computational Optimal Transport\n",
        "### Instructor: Nataliia Monina\n",
        "-----\n",
        "\n",
        "### 1. Regularization of Optimal Tranpost\n",
        "In this notebook, we'll:\n",
        "1. Utilize duality theory for entropy- (and general convex-) regularized OT and implementation of Sinkhorn from the duality point of view\n",
        "2. Explore the role of initialization (for our Sinkhorn) of the problem, any difference in solutions?\n",
        "3. Implement algorighms for OT approximation with a 'chosen' convex function $\\psi$ via gradient ascent and imitation of Sinkhorn\n",
        "4. Compare solutions (optimal plans P) achieved by different regularizations\n",
        "\n",
        "\n",
        "**Feel free to go back to `Tutorial 1` for some code snippets when you want to compare**"
      ],
      "metadata": {
        "id": "nBr5AhjDVbhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first set up some data for our experiments"
      ],
      "metadata": {
        "id": "HlUjRfjUWrDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install POT --quiet\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ot\n",
        "import ot.plot\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IQaZGmDMWqbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_1 = np.linspace(0,4, 51)\n",
        "Y_1 = np.linspace(0,4, 61)\n",
        "\n",
        "def discrete_gaussian(mean, variance, interval):\n",
        "    f = np.exp(-(interval-mean)**2/(2*variance)) / np.sqrt(2*np.pi*variance)\n",
        "    return f/f.sum()\n",
        "\n",
        "mu_1 = discrete_gaussian(1, 0.1, X_1)\n",
        "nu_1 = discrete_gaussian(2.8, 0.06, Y_1)\n",
        "\n",
        "'''Test measures'''\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(X_1, mu_1, 'o', label=\"mu_1\")\n",
        "plt.plot(Y_1, nu_1, '+', label=\"nu_1\")\n",
        "plt.legend(loc=0)\n",
        "\n",
        "'''Cost: Distance squared '''\n",
        "\n",
        "C_1 = (X_1[:, None] - Y_1[None, :]) ** 2"
      ],
      "metadata": {
        "id": "G1l6VUJjXrZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helpful function to plot and analyze our solutions\n",
        "def analyze_solutions(mu, nu, P_computed, P_for_compare, epsilon, C):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu: np.ndarray\n",
        "        Actual measure mu\n",
        "    nu: np.ndarray\n",
        "        Actual measure nu\n",
        "    P_computed: np.ndarray\n",
        "        Computed plan P with the chosen method\n",
        "    P_for_compare: np.ndarray\n",
        "        Reference plan P to compare with (e.g., output of ot.emd)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    plt.figure\n",
        "        Plot of comparisons for our computed solution\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # error of marginals\n",
        "    # mu vs. output\n",
        "    ax[0].plot(mu)\n",
        "    ax[0].plot(np.sum(P_computed, axis=1))\n",
        "    ax[0].set_title('mu vs P_computed_mu')\n",
        "    # nu vs. output\n",
        "    ax[1].plot(nu)\n",
        "    ax[1].plot(np.sum(P_computed, axis=0))\n",
        "    ax[1].set_title('nu vs P_computed_nu')\n",
        "\n",
        "    # comparison of plans\n",
        "    ax[2].imshow(P_computed)\n",
        "    ax[2].set_title(f\"P_computed (eps={epsilon})\")\n",
        "    ax[3].imshow(P_for_compare)\n",
        "    ax[3].set_title(f\"P_for_compare\")\n",
        "\n",
        "    print(\"Cost of P_computed:\", np.einsum('ij,ij', C, P_computed))\n",
        "    print(\"Cost of P_for_compare:\", np.einsum('ij,ij', C, P_for_compare))\n",
        "    return ax\n"
      ],
      "metadata": {
        "id": "HMYSMyvjtzuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sinkhorn implementation through duality theory\n",
        "\n",
        "\n",
        "\n",
        "Recall the Primal-Dual entropy regularized problems.\n",
        "\n",
        "### The regularized Primal problem\n",
        "\n",
        "Fix $\\varepsilon>0$. We are looking for a solution of\n",
        "\n",
        "$P^{\\varepsilon}\\in argmin \\{F^{\\varepsilon}_C(P) := \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1}C_{ij}P_{ij} +\\varepsilon \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1} P_{ij}(\\log P_{ij}-1)\\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\}$\n",
        "\n",
        "\n",
        "### The regularized Dual problem\n",
        "\n",
        "Consider a function $D^{\\varepsilon}_{C}:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}:$\n",
        "\n",
        "$D^{\\varepsilon}_{C}(u,v) = \\sum_{i=0}^{n-1} u_i \\mu_i + \\sum_{j=n}^{m-1} v_j \\nu_j - \\varepsilon \\sum_{i=0}^{n-1}\\sum_{j=0}^{m-1} e^{\\frac{u_i+v_j - C_{ij}}{\\varepsilon}}$\n",
        "\n",
        "#### Facts (Optimization Theory):\n",
        "1. It turns out that\n",
        "\n",
        "$$\\min \\{F^{\\varepsilon}_C(P) \\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\} = \\max \\{ D^{\\varepsilon}_C(u,v) \\,:\\, u\\in \\mathbb{R}^n,~ v\\in \\mathbb{R}^m\\}.$$\n",
        "\n",
        "2. There exists a unique minimizer $P^{\\varepsilon}$ of $F^{\\varepsilon}_C$ and it has a shape\n",
        "$$\n",
        "P^{\\varepsilon}_{ij}  = e^{\\frac{u^{\\varepsilon}_i+v^{\\varepsilon}_j - C_{ij}}{\\varepsilon}},\n",
        "$$\n",
        "where $u^{\\varepsilon}_i$ and $v^{\\varepsilon}_j$ are (any) maximizers of the Dual problem $D^{\\varepsilon}_C$.\n",
        "\n",
        "\n",
        "## Sinkhorn algorithm\n",
        "*Idea:* Do the alternate maximization of the functional $D^\\varepsilon_C$.\n",
        "\n",
        "`Start`: Initialize $(u^{(0)}, v^{(0)})$ with some values, e.g. $(0,0)$.\n",
        "\n",
        "`Iterations`: for $l=1, 2, ...$ do\n",
        "1. Compute $u^{(l)} = argmax \\{ D^{\\varepsilon}_C (u, v^{(l-1)}) \\,:\\, u \\in \\mathbb{R^n}\\}$.\n",
        "2. Compute $v^{(l)} = argmax \\{ D^{\\varepsilon}_C (u^{(l)}, v) \\,:\\, v \\in \\mathbb{R}^m\\}$.\n",
        "3. (Optional) Compute $P^{(l)} = e^{\\frac{u^{(l)}\\oplus v^{(l)} - C}{\\varepsilon}}$\n",
        "\n",
        "### (!) There are explicit formula for those $argmax D^\\varepsilon_C(\\cdot, V)$ and $argmax D^\\varepsilon_C(U, \\cdot)$ for entropic regularizaition -- compute it!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "20Ok-ZvkiHQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "Fill in the gaps in this Sinkhorn implementation\n",
        "\n",
        "Idea:\n",
        "1. Find what are the formulas $\\frac{\\partial D_C^\\varepsilon}{\\partial u_i}$ and $\\frac{\\partial D_C^\\varepsilon}{\\partial v_j}$\n",
        "2. For fixed $v$, hat is the condition for $u= argmax D_C^\\varepsilon(â‹…, v)$ ? What about $u$?"
      ],
      "metadata": {
        "id": "IwE_obpznCS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sinkhorn(mu, nu, C, v_init, eps=0.01, max_iter=100, tol=1e-6):\n",
        "    # Initialize dual variables\n",
        "    v = v_init\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        # Update rules (TODO):\n",
        "\n",
        "        '''modify this part'''\n",
        "        u = # your code\n",
        "        v = # your code\n",
        "        '''answer is at the bottom of this tutorial, but try yourself first!!!'''\n",
        "\n",
        "        P = np.exp((u[:, None]+ v[None, :] - C) / eps)\n",
        "        # Convergence check\n",
        "        if np.linalg.norm(mu - np.sum(P, axis=1), 1) < tol:\n",
        "            print('n_iters:', it)\n",
        "            break\n",
        "\n",
        "    return P, u, v\n"
      ],
      "metadata": {
        "id": "bXuq4QxnoYyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Test your Sinkhorn and compare with POT library output'''\n",
        "\n",
        "epsilon = 0.01\n",
        "P_test, u_ep, v_ep = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)"
      ],
      "metadata": {
        "id": "iyZyvF-A4Rs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_pot_sink = ot.sinkhorn(mu_1, nu_1, C_1, epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_test, P_pot_sink, epsilon, C_1)"
      ],
      "metadata": {
        "id": "PYWTlbhdJQOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "- For particular examples, try different initialization of $(u,v)$ in our implementation of Sinkhorn and compare output solutions.\n",
        "    * E.g. take initialization `(u1, v1)` and compute output of Sinkhorn `(u1_out, v1_out)`, then try a different initialization `(u2, v2)` and compute output of Sinkhorn `(u2_out, v2_out)`. Then compute differences: `lambda_u = u1_out-u2_out` and `lambda_v = v2_out-v1_out` then, plot them. What do you see?\n",
        "\n",
        "- Play around with epsilon, different marginals, costs, our $\\mathbb R^1$ or $\\mathbb R^2$ examples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iKHYnF6S8kHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment here with different regularization\n",
        "\n",
        "v_init = np.random.random(len(nu_1))-5 # for example\n",
        "P_test_ep_rand, u_ep_rand, v_ep_rand = sinkhorn(mu_1, nu_1, C_1, v_init, eps=epsilon)\n",
        "\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_test_ep_rand, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "pAkp8wVM5Mm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare solutions with different initialization\n",
        "plt.figure()\n",
        "plt.plot(u_ep-u_ep_rand)\n",
        "plt.plot(v_ep_rand-v_ep)\n",
        "plt.legend(['u1-u2', 'v2-v1'])\n",
        "\n",
        "# you can modify this part (limits of y-axis on your plot)\n",
        "# ================\n",
        "plt.ylim(-10, 10)\n",
        "# ================"
      ],
      "metadata": {
        "id": "mQ6ECHU-J6JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Free space for other experiments'''\n"
      ],
      "metadata": {
        "id": "knvcxG7j-Ppu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bqckp8x_-PuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General convex regularization and algorithm\n",
        "### Convex regularized Primal problem\n",
        "\n",
        "Fix $\\varepsilon>0$ and $\\phi:[0,\\infty)\\to\\mathbb R$ be a superlinear convex function bounded from below. We are looking for a solution of\n",
        "\n",
        "$P^{\\varepsilon}\\in argmin \\{F^{\\varepsilon}_C(P) := \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1}C_{ij}P_{ij} +\\varepsilon \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1} \\phi(P_{ij})\\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\}$\n",
        "\n",
        "\n",
        "### Convex regularized Dual problem\n",
        "\n",
        "Let $\\varepsilon>0$ and let $\\psi:\\mathbb R\\to \\mathbb R$ be a superlinear convex function bounded from below. Consider a function $D^{\\varepsilon}_{C}:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}:$\n",
        "\n",
        "$D^{\\varepsilon}_{C}(u,v) = \\sum_{i=0}^{n-1} u_i \\mu_i + \\sum_{j=n}^{m-1} v_j \\nu_j - \\varepsilon \\sum_{i=0}^{n-1}\\sum_{j=0}^{m-1} \\psi({\\frac{u_i+v_j - C_{ij}}{\\varepsilon}})$\n",
        "\n",
        "### **Facts (Optimization Theory)**:\n",
        "1. It turns out that if $\\psi(y) = \\sup\\limits_{x\\geq 0}\\{xy- \\phi(x)\\}$\n",
        "\n",
        "$$\\min \\{F^{\\varepsilon}_C(P) \\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\} = \\max \\{ D^{\\varepsilon}_C(u,v) \\,:\\, u\\in \\mathbb{R}^n,~ v\\in \\mathbb{R}^m\\}.$$\n",
        "\n",
        "2. If  $\\frac{d \\psi}{d y}$ exists, then there exists a unique minimizer $P^{\\varepsilon}$ of $F^{\\varepsilon}_C$ and it has a shape\n",
        "$$\n",
        "P^{\\varepsilon}_{ij}  = \\psi'({\\frac{u^{\\varepsilon}_i+v^{\\varepsilon}_j - C_{ij}}{\\varepsilon}}),\n",
        "$$\n",
        "where $u^{\\varepsilon}_i$ and $v^{\\varepsilon}_j$ are (any) maximizers of the Dual problem $D^{\\varepsilon}_C$.\n",
        "\n",
        "---\n",
        "For this tutorial, let's consider the following convex functions for regularization\n",
        "\n",
        "$\\psi_0(y) = \\exp(y)$\n",
        "\n",
        "$\\psi_1(y) = \\frac12 (y_+)^2 = \\begin{cases}\\frac12 y^2, & y\\geq 0 \\\\ 0, & o.w.\\end{cases}$\n",
        "\n",
        "$\\psi_2(y) = \\log(1+ \\exp(y))$\n",
        "\n"
      ],
      "metadata": {
        "id": "-lDAcNI7YbG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psi_0 = np.exp\n",
        "\n",
        "psi_1 = lambda y: (y ** 2) / 2 * (y > 0)\n",
        "\n",
        "psi_2 = lambda y: np.log(1 + np.exp(y))"
      ],
      "metadata": {
        "id": "kz8GnCLvoKRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualisation of test psi functions\n",
        "\n",
        "interval = np.linspace(-1, 1, 51)\n",
        "plt.plot(interval, psi_0(interval))\n",
        "plt.plot(interval, psi_1(interval))\n",
        "plt.plot(interval, psi_2(interval))\n",
        "plt.legend(['exp', 'quardratic', 'int_sigmoid'])\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "maIlMgNMtNl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Algorithm (with simple gradient ascent)\n",
        "*Idea:* Use gradient methods, e.g., gradient ascent to find solutions.\n",
        "\n",
        "`Start`: Initialize $(u^{(0)}, v^{(0)})$ with some values, e.g. $(0,0)$ and choose a learning rate $\\alpha$.\n",
        "\n",
        "`Iterations`: for $l=1, 2, ...$ do (for example)\n",
        "1. Compute $u^{(l)} = u^{(l-1)} + \\alpha  \\frac{\\partial D^{\\varepsilon}_C}{\\partial u} (u^{(l-1)}, v^{(l-1)})$.\n",
        "2. Compute $v^{(l)} = v^{(l-1)}+ \\alpha  \\frac{\\partial D^{\\varepsilon}_C}{\\partial v} (u^{(l)}, v^{(l-1)})$.\n",
        "3. (Optional) Compute $P^{(l)} = \\psi'({\\frac{u^{(l)}\\oplus v^{(l)} - C}{\\varepsilon}})$\n",
        "\n",
        "\n",
        "#### **Side comment:** one can, of course, also experiment with already implemented gradient optimizers (e.g. Adam) directly by specifying the variable $(u,v)=(u_1,...,u_N, v_1,..., v_M)$ and the gradient $\\nabla D^{\\varepsilon}_C = (\\frac{\\partial D^{\\varepsilon}_C}{\\partial u_1},...,\\frac{\\partial D^{\\varepsilon}_C}{\\partial u_N}, \\frac{\\partial D^{\\varepsilon}_C}{\\partial v_1}, ..., \\frac{\\partial D^{\\varepsilon}_C}{\\partial v_M})$.\n",
        "\n",
        "----\n",
        " (!) Notice that when $\\psi=exp$, then we can directly solve $(\\frac{D^{\\varepsilon}_C}{\\partial u}, \\frac{D^{\\varepsilon}_C}{\\partial v})=0$, so there is no need to do a gradient step - we can directly jump to the best candidate for a given second variable fixed.\n",
        "\n",
        "\n",
        "## Exercise:\n",
        "- Compute all necessary derivatives (pen and paper)\n",
        "- Fill in the gaps in the derivarives and the algorithm\n",
        "- Play with parameters `alpha`, `max_iter` and so on"
      ],
      "metadata": {
        "id": "8LuDtgPlruES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Answers are at the bottom of this tutorial, but try yourself first!!!'''\n",
        "\n",
        "psi_0_prime = # fill\n",
        "psi_1_prime = # fill\n",
        "psi_2_prime = # fill\n",
        "\n",
        "def dD_du(u, v, mu, C, psi_prime, eps):\n",
        "    \"\"\"returns: d D_C / d u (u, v) for chosen psi\"\"\"\n",
        "    #write your code here\n",
        "\n",
        "\n",
        "def dD_dv(u, v, nu, C, psi_prime, eps):\n",
        "    \"\"\"returns: d D_C / d v (u, v) for chosen psi\"\"\"\n",
        "    #write your code here\n",
        "\n",
        "\n",
        "def grad_ascent_D(mu, nu, C, u_init, v_init, eps=0.01, psi_prime=np.exp, alpha=0.05, max_iter=50000, tol=1e-6):\n",
        "    # Initialize dual variables\n",
        "    u = u_init\n",
        "    v = v_init\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        if (it+1) % 10000 == 0:\n",
        "            print('n_iters:', it+1)\n",
        "        # Update rules (TODO):\n",
        "\n",
        "        '''modify this part'''\n",
        "        u = # fill\n",
        "        v = # fill\n",
        "\n",
        "        P = # fill\n",
        "        '''answer is at the bottom of this tutorial, but try yourself first!!!'''\n",
        "\n",
        "        # Convergence check\n",
        "        if np.linalg.norm(mu - np.sum(P, axis=1), 1) < tol:\n",
        "            print('n_iters:', it+1)\n",
        "            break\n",
        "\n",
        "    return P, u, v\n"
      ],
      "metadata": {
        "id": "HhB5m77mmRcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on psi_0"
      ],
      "metadata": {
        "id": "Sqy5_ZV6nURq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.01\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_0, u_0, v_0 = grad_ascent_D(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_0_prime, alpha=0.1)"
      ],
      "metadata": {
        "id": "hNyYhIof4AkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with our sinkhorn, to see n_iters\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "analyze_solutions(mu_1, nu_1, P_0, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "OYHirbUJm6iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on psi_1"
      ],
      "metadata": {
        "id": "H7WN2LwGnbMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.1\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_1, u_1, v_1 = grad_ascent_D(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_1_prime, max_iter=90000, alpha=0.03)"
      ],
      "metadata": {
        "id": "FKqu7ODv4HFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_1, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "bDVFhZn0n6a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on psi_2"
      ],
      "metadata": {
        "id": "pI-Lro6QoTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.01\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_2, u_2, v_2 = grad_ascent_D(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_2_prime)"
      ],
      "metadata": {
        "id": "OoKF8Cj44HH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_2, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "6IMxBvkBoZvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One can also try imitating a Sinkhorn-type algorithm\n",
        "\n",
        "*Idea:* Do the alternate maximization of the functional $D^\\varepsilon_C$.\n",
        "\n",
        "`Start`: Initialize $(u^{(0)}, v^{(0)})$ with some values, e.g. $(0,0)$.\n",
        "\n",
        "`Iterations`: for $l=1, 2, ...$ do:\n",
        "1. Compute $u^{(l)} \\approx argmax \\{ D^{\\varepsilon}_C (u, v^{(l-1)}) \\,:\\, u \\in \\mathbb{R^n}\\}$:\n",
        "    * Run several iterations of, e.g., a gradient step. Set $u^{(l, 0)} = u^{(l-1)}$\n",
        "\n",
        "    * `Internal iterations:` for $k=1,2,...N_l$ do:\n",
        "\n",
        "        $u^{(l,k)} = u^{(l, k-1)} + \\alpha  \\frac{D^{\\varepsilon}_C}{\\partial u} (u^{(l, k-1)}, v^{(l-1)})$\n",
        "    * Set $u^{(l)} = u^{(l,N_l)}$\n",
        "\n",
        "2. Compute $v^{(l)} \\approx argmax \\{ D^{\\varepsilon}_C (u^{(l)}, v) \\,:\\, v \\in \\mathbb{R}^m\\}$ (by similar principle as for $u^{(l)})$.\n",
        "3. (Optional) Compute $P^{(l)} = \\psi'(\\frac{ u^{(l)}\\oplus v^{(l)} - C}{\\varepsilon})$\n",
        "\n",
        "### Exercises\n",
        "- Fill in the gaps\n",
        "- Play with initializations, learning rate `alpha`, `max_iter`, etc."
      ],
      "metadata": {
        "id": "CbYSjQhffgAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maximize_D_in_u(u_start, v, mu, C, eps, psi_prime, alpha=0.05, max_iter=50000, tol=1e-6):\n",
        "    u = u_start\n",
        "    for it in range(max_iter):\n",
        "        # Update rules (TODO):\n",
        "        u_prev = np.copy(u)\n",
        "\n",
        "        '''modify this part'''\n",
        "        u = # fill\n",
        "        '''answer is at the bottom of this tutorial, but try yourself first!!!'''\n",
        "        # Convergence check\n",
        "        if np.linalg.norm(u - u_prev, 1) < tol:\n",
        "            break\n",
        "    return u\n",
        "\n",
        "def maximize_D_in_v(u, v_start, nu, C, eps, psi_prime, alpha=0.05, max_iter=50000, tol=1e-6):\n",
        "    v = v_start\n",
        "    for it in range(max_iter):\n",
        "        # Update rules (TODO):\n",
        "        v_prev = np.copy(u)\n",
        "\n",
        "        '''modify this part'''\n",
        "        v = # fill\n",
        "        '''answer is at the bottom of this tutorial, but try yourself first!!!'''\n",
        "        # Convergence check\n",
        "        if np.linalg.norm(v - v_prev, 1) < tol:\n",
        "            break\n",
        "    return v\n",
        "\n",
        "def psi_sinkhorn(mu, nu, C, u_init, v_init, eps=0.01, psi_prime=np.exp, alpha=0.05, max_iter=10, tol=1e-6):\n",
        "    # Initialize dual variables\n",
        "    u = u_init\n",
        "    v = v_init\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        # Update rules (TODO):\n",
        "        print('n_iters:', it+1)\n",
        "        u = maximize_D_in_u(u, v, mu, C, eps, psi_prime, alpha)\n",
        "        v = maximize_D_in_v(u, v, nu, C, eps, psi_prime, alpha)\n",
        "\n",
        "        P = # fill\n",
        "        # Convergence check\n",
        "        if np.linalg.norm(mu - np.sum(P, axis=1), 1) < tol:\n",
        "            break\n",
        "\n",
        "    return P, u, v"
      ],
      "metadata": {
        "id": "NfQ8P90kyEuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on psi_0"
      ],
      "metadata": {
        "id": "wE4Hc1uXpWZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.01\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_0_psi_sink, u_0, v_0 = psi_sinkhorn(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_0_prime, alpha=0.1, max_iter=30)"
      ],
      "metadata": {
        "id": "TndsVh6ywEVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_0_psi_sink, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "of9_tjmupb6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on psi_1"
      ],
      "metadata": {
        "id": "GUJsd-hCpd5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.1\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_1_psi_sink, u_1, v_1 = psi_sinkhorn(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_1_prime, alpha=0.03, max_iter=13)"
      ],
      "metadata": {
        "id": "82BH60Eo4OvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_1_psi_sink, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "jy0Zcwc4q0qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test on psi_2"
      ],
      "metadata": {
        "id": "TGDLr0fFq-5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.1\n",
        "u_0 = np.zeros_like(mu_1)\n",
        "v_0 = np.zeros_like(nu_1)\n",
        "P_2_psi_sink, u_2, v_2 = psi_sinkhorn(mu_1, nu_1, C_1, u_0, v_0, epsilon, psi_2_prime, alpha=0.03, max_iter=13)"
      ],
      "metadata": {
        "id": "_hPycNgvUE8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with\n",
        "P_test, _, _ = sinkhorn(mu_1, nu_1, C_1, np.zeros_like(nu_1), eps=epsilon)\n",
        "\n",
        "analyze_solutions(mu_1, nu_1, P_2_psi_sink, P_test, epsilon, C_1)"
      ],
      "metadata": {
        "id": "PRf0SDOYrC0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "\n",
        "You have free will of what you would like to experiment with, but possible options include:\n",
        "- compute the potentials `u` and `v` with different methods (i.e., output of algorithms) and plot them to compare\n",
        "- try to generate some more measures or costs to compare, of your choice\n",
        "- you may also create some 2D examples, or just take a `mnist` dataset and try to solve OT for them using the implemented methods above\n",
        "- (more advanced) for curious individuals, take a look at other optimization methods, e.g. with adam, automatic differentiation, etc.\n"
      ],
      "metadata": {
        "id": "29y7yP0Ort_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Free space to play around on your own\"\"\""
      ],
      "metadata": {
        "id": "z5lMMz9Rrcyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iR4Hvc7HrdHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mg02D5yE4RIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (! Answers !)\n",
        "\n",
        "\n",
        "### Sinkhorn missing part to fill in:\n",
        "\n",
        "`u = eps * np.log(mu) - eps * np.log(np.sum(np.exp((v[None, :] - C) / eps), axis=1))`\n",
        "\n",
        "`v = eps * np.log(nu) - eps * np.log(np.sum(np.exp((u[:, None] - C) / eps), axis=0))`"
      ],
      "metadata": {
        "id": "jJDNR7IZ-QaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient ascent:\n",
        "\n",
        "`psi_0_prime = np.exp`\n",
        "\n",
        "`psi_1_prime = lambda y: y * (y>0)`\n",
        "\n",
        "`psi_2_prime = lambda y: 1 / (1 + np.exp(-y))`\n",
        "\n",
        "\n",
        "\n",
        "    def dD_du(u, v, mu, C, psi_prime, eps):\n",
        "        P = psi_prime((u[:, None]+ v[None, :] - C) / eps)\n",
        "        return mu - np.sum(P, axis=1)\n",
        "` `\n",
        "\n",
        "    def dD_dv(u, v, nu, C, psi_prime, eps):   \n",
        "        P = psi_prime((u[:, None]+ v[None, :] - C) / eps)\n",
        "        return nu - np.sum(P, axis=0)`\n",
        "\n",
        "\n",
        "in `grad_ascent_D` the missing part is:\n",
        "\n",
        "    ...\n",
        "        u = u + alpha * dD_du(u, v, mu, C, psi_prime, eps)\n",
        "        v = v + alpha * dD_dv(u, v, nu, C, psi_prime, eps)\n",
        "    ...\n",
        "\n",
        "        P = psi_prime((u[:, None]+ v[None, :] - C) / eps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "02UeFS-e4PAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psi-Sinkhorn:\n",
        "\n",
        "in `def maximize_D_in_u` the missing part:\n",
        "\n",
        "        ...\n",
        "        u = u + alpha * dD_du(u, v, mu, C, psi_prime, eps)\n",
        "        ...\n",
        "\n",
        "similar idea for `def maximize_D_in_v`.\n",
        "\n",
        "then, in `psi_sinkhorn`:\n",
        "\n",
        "        ...\n",
        "        P = psi_prime((u[:, None]+ v[None, :] - C) / eps)\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "IyXAi5ad_xwx"
      }
    }
  ]
}